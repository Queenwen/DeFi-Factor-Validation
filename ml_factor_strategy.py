import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from statsmodels.regression.linear_model import OLS
from statsmodels.tools import add_constant
import glob
from datetime import datetime, timedelta
import os

# ä»ç¯å¢ƒå˜é‡è·å–å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
def get_env_var(var_name, default_value):
    return os.environ.get(var_name, default_value)

# è®¾ç½®è¾“å‡ºç›®å½•
output_dir = get_env_var('OUTPUT_DIR', '/Users/queenwen/Desktop/QI_Paper/ml_strategy_results')
os.makedirs(output_dir, exist_ok=True)

# æ•°æ®æºç›®å½•
data_dir = '/Users/queenwen/Desktop/QI_Paper'
normalized_data_dir = os.path.join(data_dir, 'normalized_factors', 'IC_analysis')
return_data_dir = os.path.join(data_dir, 'return_data')
market_return_dir = os.path.join(return_data_dir, 'market_returns')

# ä»ç¯å¢ƒå˜é‡è·å–å‚æ•°
return_period = get_env_var('HOLDING_PERIOD', '3d')
model_type = get_env_var('MODEL_TYPE', 'lasso')
rolling_window = int(get_env_var('ROLLING_WINDOW', '30'))
prediction_window = int(get_env_var('PREDICTION_WINDOW', '3'))
top_pct = float(get_env_var('TOP_PCT', '0.2'))
bottom_pct = float(get_env_var('BOTTOM_PCT', '0.2'))

# è·å–ç­›é€‰åçš„é«˜è§£é‡ŠåŠ›å› å­
def get_available_factors():
    """è·å–ç­›é€‰åçš„é«˜è§£é‡ŠåŠ›å› å­"""
    # ç­›é€‰å‡ºçš„é«˜è§£é‡ŠåŠ›å› å­åˆ—è¡¨
    selected_factors = [
        'exchange_inflow_usd',
        'social_volume_total', 
        'whale_transaction_count_100k_usd_to_inf',
        'sentiment_weighted_total_1d',
        'whale_transaction_volume_100k_usd_to_inf',
        'exchange_outflow_usd'  # åå‘å› å­
    ]
    
    factors = []
    
    # åªåŠ è½½ç­›é€‰å‡ºçš„å› å­
    for factor_name in selected_factors:
        factor_file = f"{factor_name}.csv"
        factor_path = os.path.join(normalized_data_dir, factor_file)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(factor_path):
            factors.append((factor_name, factor_path))
            print(f"âœ… æ‰¾åˆ°å› å­æ–‡ä»¶: {factor_name}")
        else:
            print(f"âš ï¸  è­¦å‘Š: å› å­æ–‡ä»¶ä¸å­˜åœ¨: {factor_path}")
    
    print(f"\nğŸ“Š å…±ç­›é€‰å‡º {len(factors)} ä¸ªé«˜è§£é‡ŠåŠ›å› å­ç”¨äºæœºå™¨å­¦ä¹ ")
    return factors

# åŠ è½½å› å­æ•°æ®
def load_factor_data(factor_path):
    """åŠ è½½å› å­æ•°æ®"""
    df = pd.read_csv(factor_path)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df.set_index('datetime', inplace=True)
    return df

# åŠ è½½æ”¶ç›Šæ•°æ®
def load_return_data(return_period):
    """åŠ è½½æ”¶ç›Šæ•°æ®"""
    return_file = os.path.join(return_data_dir, f'returns_{return_period}.csv')
    df = pd.read_csv(return_file)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df.set_index('datetime', inplace=True)
    
    # åªä¿ç•™log_returnåˆ—
    log_return_cols = [col for col in df.columns if f'log_return_{return_period}' in col]
    return df[log_return_cols]

# åŠ è½½å¸‚åœºæ”¶ç›Šæ•°æ®
def load_market_return_data(return_period):
    """åŠ è½½å¸‚åœºæ”¶ç›Šæ•°æ®"""
    market_return_file = os.path.join(market_return_dir, f'market_log_return_{return_period}.csv')
    df = pd.read_csv(market_return_file)
    df['date'] = pd.to_datetime(df['date'])
    df.set_index('date', inplace=True)
    return df

# å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®
def prepare_model_data(
    factors_data, return_data, rolling_window=30, fixed_tokens=None):
    """å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®"""
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if not factors_data or return_data.empty:
        print("é”™è¯¯: è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•å‡†å¤‡æ¨¡å‹æ•°æ®")
        return np.array([]).reshape(0, 0), np.array([]), [], [], None
    
    # åˆå¹¶æ‰€æœ‰å› å­æ•°æ®
    factor_dfs = []
    
    for factor_name, factor_df in factors_data:
        # é‡å‘½ååˆ—ä»¥åŒ…å«å› å­åç§°
        renamed_df = factor_df.copy()
        renamed_df.columns = [f'{factor_name}_{col.split("_")[0]}' for col in factor_df.columns]
        factor_dfs.append(renamed_df)
    
    # ä½¿ç”¨concatä¸€æ¬¡æ€§åˆå¹¶æ‰€æœ‰å› å­æ•°æ®
    all_factors = pd.concat(factor_dfs, axis=1, sort=False)
    all_factors = all_factors.loc[:, ~all_factors.columns.duplicated()]  # å»é™¤é‡å¤åˆ—
    
    # å¯¹å› å­æ•°æ®è¿›è¡Œæ¸…ç†å’Œæ ‡å‡†åŒ–å¤„ç†
    print(f"æ¸…ç†å› å­æ•°æ®ï¼ŒåŸå§‹å½¢çŠ¶: {all_factors.shape}")
    # æ›¿æ¢æ— ç©·å¤§å€¼å’Œæå¤§å€¼
    all_factors = all_factors.replace([np.inf, -np.inf], np.nan)
    # å¯¹æå¤§å€¼è¿›è¡Œæˆªæ–­ï¼ˆ99.9%åˆ†ä½æ•°ï¼‰
    for col in all_factors.columns:
        if all_factors[col].dtype in ['float64', 'float32', 'int64', 'int32']:
            upper_bound = all_factors[col].quantile(0.999)
            lower_bound = all_factors[col].quantile(0.001)
            all_factors[col] = all_factors[col].clip(lower=lower_bound, upper=upper_bound)
    
    # å¡«å……ç¼ºå¤±å€¼
    all_factors = all_factors.fillna(0)
    
    # æ ‡å‡†åŒ–å¤„ç†
    scaler = StandardScaler()
    all_factors_scaled = pd.DataFrame(
        scaler.fit_transform(all_factors),
        index=all_factors.index,
        columns=all_factors.columns
    )
    
    # å†æ¬¡æ£€æŸ¥å¹¶æ¸…ç†æ ‡å‡†åŒ–åçš„æ•°æ®
    all_factors_scaled = all_factors_scaled.replace([np.inf, -np.inf], 0)
    all_factors_scaled = all_factors_scaled.fillna(0)
    
    # åˆå¹¶å› å­æ•°æ®å’Œæ”¶ç›Šæ•°æ®
    merged_data = pd.merge(all_factors_scaled, return_data, left_index=True, right_index=True, how='inner')
    
    # å¦‚æœæ²¡æœ‰æä¾›å›ºå®šä»£å¸åˆ—è¡¨ï¼Œåˆ™ä»æ•°æ®ä¸­æå–
    if fixed_tokens is None:
        # è·å–æ‰€æœ‰å¯ç”¨çš„ä»£å¸
        return_columns = [col for col in merged_data.columns if 'log_return' in col]
        fixed_tokens = sorted(list(set([col.split('_')[0] for col in return_columns])))
        print(f"æ£€æµ‹åˆ° {len(fixed_tokens)} ä¸ªä»£å¸: {fixed_tokens[:10]}...")
    
    # åˆ›å»ºè®­ç»ƒé›†
    X = []
    y = []
    dates = []
    tokens = []
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
    if len(merged_data.index) <= rolling_window:
        print(f"è­¦å‘Š: æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•å‡†å¤‡è®­ç»ƒæ•°æ®ã€‚æ•°æ®é•¿åº¦: {len(merged_data.index)}, éœ€è¦è‡³å°‘: {rolling_window + 1}")
        return np.array([]).reshape(0, 0), np.array([]), [], [], fixed_tokens
    
    for date in merged_data.index[rolling_window:]:
        try:
            # è·å–å½“å‰æ—¥æœŸä¹‹å‰çš„rolling_windowå¤©çš„æ•°æ®ä½œä¸ºç‰¹å¾
            window_data = merged_data.loc[:date].iloc[-rolling_window-1:-1]
            
            # è·å–å½“å‰æ—¥æœŸçš„æ”¶ç›Šä½œä¸ºç›®æ ‡
            current_returns = merged_data.loc[date, [col for col in merged_data.columns if 'log_return' in col]]
            
            for token in fixed_tokens:
                return_col = f'{token}_log_return'
                if return_col in current_returns.index:
                    # è·å–è¯¥ä»£å¸åœ¨çª—å£æœŸå†…çš„æ‰€æœ‰å› å­å€¼ï¼ŒæŒ‰å›ºå®šé¡ºåº
                    token_feature_cols = []
                    for factor_name, _ in factors_data:
                        factor_col = f'{factor_name}_{token}'
                        if factor_col in window_data.columns:
                            token_feature_cols.append(factor_col)
                    
                    if token_feature_cols:
                        token_features = window_data[token_feature_cols]
                        feature_vector = token_features.values.flatten()
                        
                        # æ£€æŸ¥ç‰¹å¾å‘é‡æ˜¯å¦åŒ…å«NaN
                        if not np.isnan(feature_vector).any() and not np.isnan(current_returns[return_col]):
                            X.append(feature_vector)
                            y.append(current_returns[return_col])
                            dates.append(date)
                            tokens.append(token)
        except Exception as e:
            print(f"å¤„ç†æ—¥æœŸ {date} æ—¶å‡ºé”™: {str(e)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬
    if len(X) == 0:
        print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬")
        return np.array([]).reshape(0, 0), np.array([]), [], [], fixed_tokens
    
    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾å‘é‡é•¿åº¦ä¸€è‡´
    feature_lengths = [len(x) for x in X]
    if len(set(feature_lengths)) > 1:
        print(f"è­¦å‘Š: ç‰¹å¾å‘é‡é•¿åº¦ä¸ä¸€è‡´: {set(feature_lengths)}")
        # æ‰¾å‡ºæœ€å¸¸è§çš„é•¿åº¦
        from collections import Counter
        most_common_length = Counter(feature_lengths).most_common(1)[0][0]
        print(f"ä½¿ç”¨æœ€å¸¸è§çš„ç‰¹å¾é•¿åº¦: {most_common_length}")
        # åªä¿ç•™é•¿åº¦ä¸€è‡´çš„æ ·æœ¬
        valid_indices = [i for i, length in enumerate(feature_lengths) if length == most_common_length]
        X = [X[i] for i in valid_indices]
        y = [y[i] for i in valid_indices]
        dates = [dates[i] for i in valid_indices]
        tokens = [tokens[i] for i in valid_indices]
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    X_array = np.array(X)
    y_array = np.array(y)
    
    print(f"å‡†å¤‡äº† {len(X_array)} ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç‰¹å¾ç»´åº¦: {X_array.shape[1] if X_array.shape[0] > 0 else 0}")
    
    return X_array, y_array, dates, tokens, fixed_tokens

# è®­ç»ƒLassoæ¨¡å‹
def train_lasso_model(X, y, alpha=0.01):
    """è®­ç»ƒLassoæ¨¡å‹"""
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if X is None or y is None:
        print("é”™è¯¯: è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒLassoæ¨¡å‹")
        return None
    
    if X.shape[0] == 0 or y.shape[0] == 0:
        raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
    
    # æ£€æŸ¥Xå’Œyçš„é•¿åº¦æ˜¯å¦ä¸€è‡´
    if X.shape[0] != y.shape[0]:
        raise ValueError(f"Xå’Œyçš„é•¿åº¦ä¸ä¸€è‡´: X.shape={X.shape}, y.shape={y.shape}")
    
    try:
        # å°è¯•ä¸åŒçš„alphaå€¼ï¼Œå¦‚æœæ¨¡å‹ä¸æ”¶æ•›
        alphas = [alpha, 0.1, 1.0, 10.0]
        for a in alphas:
            try:
                model = Lasso(alpha=a, max_iter=10000, tol=1e-3)
                model.fit(X, y)
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¶æ•›
                if model.n_iter_ < 10000:
                    print(f"Lassoæ¨¡å‹ä½¿ç”¨alpha={a}æ”¶æ•›ï¼Œè¿­ä»£æ¬¡æ•°: {model.n_iter_}")
                    return model
                else:
                    print(f"Lassoæ¨¡å‹ä½¿ç”¨alpha={a}æœªæ”¶æ•›ï¼Œå°è¯•æ›´å¤§çš„alphaå€¼")
            except Exception as e:
                print(f"ä½¿ç”¨alpha={a}è®­ç»ƒLassoæ¨¡å‹å¤±è´¥: {str(e)}")
        
        # å¦‚æœæ‰€æœ‰alphaå€¼éƒ½ä¸æ”¶æ•›ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå°è¯•çš„alphaå€¼
        print(f"æ‰€æœ‰alphaå€¼éƒ½æœªèƒ½ä½¿æ¨¡å‹æ”¶æ•›ï¼Œä½¿ç”¨alpha={alphas[-1]}")
        model = Lasso(alpha=alphas[-1], max_iter=10000, tol=1e-2)
        model.fit(X, y)
        return model
    except Exception as e:
        raise ValueError(f"è®­ç»ƒLassoæ¨¡å‹å¤±è´¥: {str(e)}")

# è®­ç»ƒLightGBMæ¨¡å‹
def train_lightgbm_model(X, y):
    """è®­ç»ƒLightGBMæ¨¡å‹"""
    try:
        # æ¡ä»¶å¯¼å…¥lightgbm
        try:
            import lightgbm as lgb
        except ImportError:
            raise ImportError("LightGBMæœªå®‰è£…æˆ–æ— æ³•å¯¼å…¥ã€‚è¯·å®‰è£…LightGBM: pip install lightgbm")
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®
        if X is None or y is None:
            print("é”™è¯¯: è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒLightGBMæ¨¡å‹")
            return None
        
        if X.shape[0] == 0 or y.shape[0] == 0:
            raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
        
        # æ£€æŸ¥Xå’Œyçš„é•¿åº¦æ˜¯å¦ä¸€è‡´
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"Xå’Œyçš„é•¿åº¦ä¸ä¸€è‡´: X.shape={X.shape}, y.shape={y.shape}")
        
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }
        
        # å¦‚æœæ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œå‡å°‘æ¨¡å‹å¤æ‚åº¦
        if X.shape[0] < 100:
            print(f"æ ·æœ¬æ•°é‡è¾ƒå°‘ ({X.shape[0]}), å‡å°‘æ¨¡å‹å¤æ‚åº¦")
            params.update({
                'num_leaves': 7,
                'min_data_in_leaf': 3,
                'learning_rate': 0.01
            })
        
        train_data = lgb.Dataset(X, label=y)
        model = lgb.train(params, train_data, num_boost_round=100)
        return model
    except Exception as e:
        raise ValueError(f"è®­ç»ƒLightGBMæ¨¡å‹å¤±è´¥: {str(e)}")

# æ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹
def rolling_train_predict(factors_data, return_data, model_type='lasso', rolling_window=30, prediction_window=30):
    """æ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹"""
    print("å¼€å§‹å‡†å¤‡å› å­æ•°æ®...")
    # åˆå¹¶æ‰€æœ‰å› å­æ•°æ®
    factor_dfs = []
    
    for factor_name, factor_df in factors_data:
        # é‡å‘½ååˆ—ä»¥åŒ…å«å› å­åç§°
        renamed_df = factor_df.copy()
        renamed_df.columns = [f'{factor_name}_{col.split("_")[0]}' for col in factor_df.columns]
        factor_dfs.append(renamed_df)
    
    # ä½¿ç”¨concatä¸€æ¬¡æ€§åˆå¹¶æ‰€æœ‰å› å­æ•°æ®
    all_factors = pd.concat(factor_dfs, axis=1, sort=False)
    all_factors = all_factors.loc[:, ~all_factors.columns.duplicated()]  # å»é™¤é‡å¤åˆ—
    
    # å¯¹å› å­æ•°æ®è¿›è¡Œæ¸…ç†å’Œæ ‡å‡†åŒ–å¤„ç†
    print(f"æ¸…ç†å› å­æ•°æ®ï¼ŒåŸå§‹å½¢çŠ¶: {all_factors.shape}")
    # æ›¿æ¢æ— ç©·å¤§å€¼å’Œæå¤§å€¼
    all_factors = all_factors.replace([np.inf, -np.inf], np.nan)
    # å¯¹æå¤§å€¼è¿›è¡Œæˆªæ–­ï¼ˆ99.9%åˆ†ä½æ•°ï¼‰
    for col in all_factors.columns:
        if all_factors[col].dtype in ['float64', 'float32', 'int64', 'int32']:
            upper_bound = all_factors[col].quantile(0.999)
            lower_bound = all_factors[col].quantile(0.001)
            all_factors[col] = all_factors[col].clip(lower=lower_bound, upper=upper_bound)
    
    # å¡«å……ç¼ºå¤±å€¼
    all_factors = all_factors.fillna(0)
    
    # æ ‡å‡†åŒ–å¤„ç†
    print(f"å¯¹å› å­æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œå½¢çŠ¶: {all_factors.shape}...")
    scaler = StandardScaler()
    all_factors_scaled = pd.DataFrame(
        scaler.fit_transform(all_factors),
        index=all_factors.index,
        columns=all_factors.columns
    )
    
    # å†æ¬¡æ£€æŸ¥å¹¶æ¸…ç†æ ‡å‡†åŒ–åçš„æ•°æ®
    all_factors_scaled = all_factors_scaled.replace([np.inf, -np.inf], 0)
    all_factors_scaled = all_factors_scaled.fillna(0)
    
    # åˆå¹¶å› å­æ•°æ®å’Œæ”¶ç›Šæ•°æ®
    print("åˆå¹¶å› å­æ•°æ®å’Œæ”¶ç›Šæ•°æ®...")
    merged_data = pd.merge(all_factors_scaled, return_data, left_index=True, right_index=True, how='inner')
    print(f"åˆå¹¶åæ•°æ®å½¢çŠ¶: {merged_data.shape}")
    
    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    predictions = []
    actual_returns = []
    dates = []
    tokens = []
    model_weights = []
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿè¿›è¡Œæ»šåŠ¨è®­ç»ƒ
    if len(merged_data.index) <= rolling_window + prediction_window:
        print(f"è­¦å‘Š: æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ»šåŠ¨è®­ç»ƒã€‚æ•°æ®é•¿åº¦: {len(merged_data.index)}, éœ€è¦è‡³å°‘: {rolling_window + prediction_window + 1}")
        # åˆ›å»ºç©ºçš„ç»“æœDataFrame
        results = pd.DataFrame({
            'date': [],
            'token': [],
            'predicted_return': [],
            'actual_return': []
        })
        return results, []
    
    # æ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹
    print("å¼€å§‹æ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹...")
    successful_windows = 0
    total_windows = (len(merged_data.index) - rolling_window) // prediction_window
    print(f"å°†æ‰§è¡Œ {total_windows} ä¸ªæ»šåŠ¨çª—å£çš„è®­ç»ƒå’Œé¢„æµ‹")
    
    for i in range(rolling_window, len(merged_data.index) - prediction_window, prediction_window):
        try:
            print(f"è®­ç»ƒçª—å£: {i-rolling_window} åˆ° {i}")
            train_end_idx = i
            test_start_idx = i
            test_end_idx = min(i + prediction_window, len(merged_data.index))
            
            train_data = merged_data.iloc[:train_end_idx]
            test_data = merged_data.iloc[test_start_idx:test_end_idx]
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X_train, y_train, train_dates, train_tokens, fixed_tokens = prepare_model_data(
                [(factor_name, factor_df.loc[train_data.index]) for factor_name, factor_df in factors_data],
                return_data.loc[train_data.index],
                rolling_window
            )
            
            # æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦ä¸ºç©º
            if len(X_train) == 0 or len(y_train) == 0:
                print("è­¦å‘Š: è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ­¤è®­ç»ƒçª—å£")
                continue
                
            print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: X_train={X_train.shape}, y_train={y_train.shape}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
            if np.isnan(X_train).any() or np.isnan(y_train).any():
                print("è­¦å‘Š: è®­ç»ƒæ•°æ®åŒ…å«NaNå€¼ï¼Œå°è¯•åˆ é™¤æˆ–å¡«å……...")
                # æ‰¾å‡ºåŒ…å«NaNçš„è¡Œ
                nan_rows = np.isnan(X_train).any(axis=1) | np.isnan(y_train)
                print(f"å‘ç° {nan_rows.sum()} è¡ŒåŒ…å«NaNå€¼")
                
                # åˆ é™¤åŒ…å«NaNçš„è¡Œ
                X_train = X_train[~nan_rows]
                y_train = y_train[~nan_rows]
                
                # å†æ¬¡æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦è¶³å¤Ÿ
                if len(X_train) < 10:  # è‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                    print(f"è­¦å‘Š: åˆ é™¤NaNåè®­ç»ƒæ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡æ­¤çª—å£")
                    continue
            
            # è®­ç»ƒæ¨¡å‹
            try:
                if model_type == 'lasso':
                    model = train_lasso_model(X_train, y_train)
                    # ä¿å­˜æ¨¡å‹æƒé‡
                    current_weights = model.coef_
                elif model_type == 'lightgbm':
                    model = train_lightgbm_model(X_train, y_train)
                    # ä¿å­˜æ¨¡å‹æƒé‡ï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰
                    current_weights = model.feature_importance(importance_type='gain')
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                
                model_weights.append(current_weights)
                successful_windows += 1
            except Exception as e:
                print(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
                continue
            
            # å¯¹æµ‹è¯•é›†ä¸­çš„æ¯ä¸€å¤©è¿›è¡Œé¢„æµ‹
            print(f"é¢„æµ‹æµ‹è¯•é›†: {test_start_idx} åˆ° {test_end_idx}")
            for test_date in test_data.index:
                # è·å–å½“å‰æ—¥æœŸä¹‹å‰çš„rolling_windowå¤©çš„æ•°æ®ä½œä¸ºç‰¹å¾
                try:
                    window_end_date = test_data.index[test_data.index < test_date][-1] if any(test_data.index < test_date) else train_data.index[-1]
                    window_start_idx = list(merged_data.index).index(window_end_date) - rolling_window + 1
                    window_end_idx = list(merged_data.index).index(window_end_date) + 1
                    window_data = merged_data.iloc[window_start_idx:window_end_idx]
                    
                    # è·å–å½“å‰æ—¥æœŸçš„å®é™…æ”¶ç›Š
                    current_returns = test_data.loc[test_date, [col for col in test_data.columns if 'log_return' in col]]
                    
                    # å¯¹æ¯ä¸ªå›ºå®šä»£å¸è¿›è¡Œé¢„æµ‹
                    for token in fixed_tokens:
                        return_col = f'{token}_log_return'
                        if return_col in current_returns.index:
                            # è·å–è¯¥ä»£å¸åœ¨çª—å£æœŸå†…çš„æ‰€æœ‰å› å­å€¼ï¼ŒæŒ‰å›ºå®šé¡ºåº
                            token_feature_cols = []
                            for factor_name, _ in factors_data:
                                factor_col = f'{factor_name}_{token}'
                                if factor_col in window_data.columns:
                                    token_feature_cols.append(factor_col)
                            
                            if token_feature_cols:
                                try:
                                    token_features = window_data[token_feature_cols]
                                    features_flat = token_features.values.flatten()
                                    # ç¡®ä¿ç‰¹å¾å‘é‡æ˜¯äºŒç»´çš„
                                    features_2d = features_flat.reshape(1, -1)
                                    
                                    # æ£€æŸ¥ç‰¹å¾å‘é‡æ˜¯å¦åŒ…å«NaN
                                    if np.isnan(features_2d).any():
                                        continue
                                    
                                    # æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´
                                    if features_2d.shape[1] != X_train.shape[1]:
                                        continue
                                    
                                    if model_type == 'lasso':
                                        pred = model.predict(features_2d)[0]
                                    elif model_type == 'lightgbm':
                                        pred = model.predict(features_2d)[0]
                                    
                                    predictions.append(pred)
                                    actual_returns.append(current_returns[return_col])
                                    dates.append(test_date)
                                    tokens.append(token)
                                except Exception as e:
                                    continue  # é™é»˜è·³è¿‡é”™è¯¯ï¼Œé¿å…è¿‡å¤šè¾“å‡º
                except Exception as e:
                    print(f"å¤„ç†æµ‹è¯•æ—¥æœŸ {test_date} æ—¶å‡ºé”™: {str(e)}")
            
            # æ¯10ä¸ªçª—å£æ‰“å°ä¸€æ¬¡è¿›åº¦
            if successful_windows % 10 == 0 or successful_windows == total_windows:
                print(f"è¿›åº¦: {successful_windows}/{total_windows} ä¸ªçª—å£å®Œæˆ")
                
        except Exception as e:
            print(f"å¤„ç†è®­ç»ƒçª—å£ {i} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„é¢„æµ‹
    if len(predictions) == 0:
        print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸçš„é¢„æµ‹ç»“æœï¼Œè¿”å›ç©ºDataFrame")
        results = pd.DataFrame({
            'date': [],
            'token': [],
            'predicted_return': [],
            'actual_return': []
        })
        return results, model_weights
    
    # åˆ›å»ºç»“æœDataFrame
    print(f"é¢„æµ‹å®Œæˆï¼Œå…± {len(predictions)} æ¡é¢„æµ‹è®°å½•")
    results = pd.DataFrame({
        'date': dates,
        'token': tokens,
        'predicted_return': predictions,
        'actual_return': actual_returns
    })
    
    return results, model_weights

# æ„å»ºå¤šç©ºç»„åˆ
def build_long_short_portfolio(predictions_df, top_pct=0.2, bottom_pct=0.2):
    """æ„å»ºå¤šç©ºç»„åˆ"""
    portfolio_returns = []
    dates = []
    
    # æ£€æŸ¥é¢„æµ‹ç»“æœæ˜¯å¦ä¸ºç©º
    if predictions_df.empty:
        print("è­¦å‘Š: é¢„æµ‹ç»“æœä¸ºç©ºï¼Œæ— æ³•æ„å»ºæŠ•èµ„ç»„åˆ")
        # è¿”å›ç©ºçš„DataFrameï¼Œä½†åŒ…å«å¿…è¦çš„åˆ—
        return pd.DataFrame(columns=['long_return', 'short_return', 'long_short_return'])
    
    print(f"æ„å»ºå¤šç©ºç»„åˆ: åšå¤šå‰ {top_pct*100}%ï¼Œåšç©ºå {bottom_pct*100}%")
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    for date, group in predictions_df.groupby('date'):
        try:
            # æ£€æŸ¥å½“å‰æ—¥æœŸçš„æ ·æœ¬æ•°é‡
            n_tokens = len(group)
            if n_tokens < 5:  # è‡³å°‘éœ€è¦5ä¸ªä»£å¸æ‰èƒ½æ„å»ºæœ‰æ„ä¹‰çš„ç»„åˆ
                print(f"è­¦å‘Š: æ—¥æœŸ {date} çš„ä»£å¸æ•°é‡ä¸è¶³ ({n_tokens})ï¼Œè·³è¿‡")
                continue
            
            # æŒ‰é¢„æµ‹æ”¶ç›Šæ’åº
            sorted_group = group.sort_values('predicted_return', ascending=False)
            
            # è®¡ç®—åˆ†ä½æ•°ä½ç½®
            top_n = max(1, int(n_tokens * top_pct))  # è‡³å°‘é€‰æ‹©1ä¸ªä»£å¸
            bottom_n = max(1, int(n_tokens * bottom_pct))  # è‡³å°‘é€‰æ‹©1ä¸ªä»£å¸
            
            print(f"æ—¥æœŸ {date}: æ€»ä»£å¸æ•° {n_tokens}, åšå¤š {top_n} ä¸ª, åšç©º {bottom_n} ä¸ª")
            
            # é€‰æ‹©åšå¤šå’Œåšç©ºçš„ä»£å¸
            long_tokens = sorted_group.iloc[:top_n]
            short_tokens = sorted_group.iloc[-bottom_n:]
            
            # è®¡ç®—å¤šç©ºç»„åˆæ”¶ç›Šï¼ˆç­‰æƒé‡ï¼‰
            if not long_tokens.empty and not short_tokens.empty:
                # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
                if long_tokens['actual_return'].isna().any() or short_tokens['actual_return'].isna().any():
                    print(f"è­¦å‘Š: æ—¥æœŸ {date} çš„æ”¶ç›Šæ•°æ®åŒ…å«NaNå€¼ï¼Œä½¿ç”¨dropnaå¤„ç†")
                    long_return = long_tokens['actual_return'].dropna().mean()
                    short_return = short_tokens['actual_return'].dropna().mean()
                else:
                    long_return = long_tokens['actual_return'].mean()
                    short_return = short_tokens['actual_return'].mean()
                
                # å¦‚æœä»ç„¶æœ‰NaNå€¼ï¼Œè·³è¿‡è¿™ä¸ªæ—¥æœŸ
                if np.isnan(long_return) or np.isnan(short_return):
                    print(f"è­¦å‘Š: æ—¥æœŸ {date} çš„æ”¶ç›Šè®¡ç®—ç»“æœä¸ºNaNï¼Œè·³è¿‡")
                    continue
                
                long_short_return = long_return - short_return
                
                portfolio_returns.append({
                    'date': date,
                    'long_return': long_return,
                    'short_return': short_return,
                    'long_short_return': long_short_return
                })
                dates.append(date)
        except Exception as e:
            print(f"å¤„ç†æ—¥æœŸ {date} æ—¶å‡ºé”™: {str(e)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„äº¤æ˜“æ—¥
    if len(portfolio_returns) == 0:
        print("è­¦å‘Š: æ²¡æœ‰è¶³å¤Ÿçš„äº¤æ˜“æ—¥æ„å»ºæŠ•èµ„ç»„åˆ")
        return pd.DataFrame(columns=['long_return', 'short_return', 'long_short_return'])
    
    # åˆ›å»ºç»“æœDataFrame
    portfolio_df = pd.DataFrame(portfolio_returns)
    portfolio_df.set_index('date', inplace=True)
    portfolio_df.sort_index(inplace=True)  # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
    
    print(f"å¤šç©ºç»„åˆæ„å»ºå®Œæˆï¼Œå…± {len(portfolio_df)} ä¸ªäº¤æ˜“æ—¥")
    return portfolio_df

# è®¡ç®—ç´¯ç§¯æ”¶ç›Š
def calculate_cumulative_returns(portfolio_df):
    """è®¡ç®—ç´¯ç§¯æ”¶ç›Š"""
    portfolio_df['cum_long_return'] = np.exp(np.cumsum(portfolio_df['long_return'])) - 1
    portfolio_df['cum_short_return'] = np.exp(np.cumsum(portfolio_df['short_return'])) - 1
    portfolio_df['cum_long_short_return'] = np.exp(np.cumsum(portfolio_df['long_short_return'])) - 1
    
    return portfolio_df

# ç»˜åˆ¶ç´¯ç§¯æ”¶ç›Šå›¾
def plot_cumulative_returns(portfolio_df, title="Cumulative Returns"):
    """ç»˜åˆ¶ç´¯ç§¯æ”¶ç›Šå›¾"""
    plt.figure(figsize=(12, 6))
    plt.plot(portfolio_df.index, portfolio_df['cum_long_return'], label='Long Portfolio')
    plt.plot(portfolio_df.index, portfolio_df['cum_short_return'], label='Short Portfolio')
    plt.plot(portfolio_df.index, portfolio_df['cum_long_short_return'], label='Long-Short Portfolio')
    
    plt.title(title)
    plt.xlabel('Date')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid(True)
    
    # ä¿å­˜å›¾è¡¨
    save_path = os.path.join(output_dir, 'cumulative_returns.png')
    plt.savefig(save_path)
    plt.close()
    print(f"âœ… ç´¯ç§¯æ”¶ç›Šå›¾å·²ä¿å­˜è‡³: {save_path}")

# è®¡ç®—æŠ•èµ„ç»„åˆç»Ÿè®¡æŒ‡æ ‡
def calculate_portfolio_stats(portfolio_df, market_returns):
    """è®¡ç®—æŠ•èµ„ç»„åˆç»Ÿè®¡æŒ‡æ ‡"""
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if portfolio_df is None or portfolio_df.empty:
        print("è­¦å‘Š: æŠ•èµ„ç»„åˆæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡")
        return pd.DataFrame({
            'Portfolio': ['Long', 'Short', 'Long-Short', 'Market'],
            'Annual Return': [np.nan, np.nan, np.nan, np.nan],
            'Annual Volatility': [np.nan, np.nan, np.nan, np.nan],
            'Sharpe Ratio': [np.nan, np.nan, np.nan, np.nan],
            'Max Drawdown': [np.nan, np.nan, np.nan, np.nan]
        })
    
    if market_returns is None or market_returns.empty:
        print("è­¦å‘Š: å¸‚åœºæ”¶ç›Šæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡")
        return pd.DataFrame({
            'Portfolio': ['Long', 'Short', 'Long-Short', 'Market'],
            'Annual Return': [np.nan, np.nan, np.nan, np.nan],
            'Annual Volatility': [np.nan, np.nan, np.nan, np.nan],
            'Sharpe Ratio': [np.nan, np.nan, np.nan, np.nan],
            'Max Drawdown': [np.nan, np.nan, np.nan, np.nan]
        })
    
    try:
        # åˆå¹¶æŠ•èµ„ç»„åˆæ”¶ç›Šå’Œå¸‚åœºæ”¶ç›Š
        merged_df = pd.merge(portfolio_df, market_returns, left_index=True, right_index=True, how='inner')
        
        # æ£€æŸ¥åˆå¹¶åçš„æ•°æ®æ˜¯å¦ä¸ºç©º
        if merged_df.empty:
            print("è­¦å‘Š: æŠ•èµ„ç»„åˆå’Œå¸‚åœºæ”¶ç›Šæ²¡æœ‰å…±åŒçš„æ—¥æœŸ")
            return pd.DataFrame({
                'Portfolio': ['Long', 'Short', 'Long-Short', 'Market'],
                'Annual Return': [np.nan, np.nan, np.nan, np.nan],
                'Annual Volatility': [np.nan, np.nan, np.nan, np.nan],
                'Sharpe Ratio': [np.nan, np.nan, np.nan, np.nan],
                'Max Drawdown': [np.nan, np.nan, np.nan, np.nan]
            })
        
        # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
        try:
            n_days = (merged_df.index[-1] - merged_df.index[0]).days
            if n_days <= 0:  # å¦‚æœåªæœ‰ä¸€å¤©æ•°æ®
                n_days = 1
            annualized_factor = 365 / n_days
        except Exception as e:
            print(f"è®¡ç®—äº¤æ˜“å¤©æ•°æ—¶å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨æ•°æ®ç‚¹æ•°é‡ä»£æ›¿")
            n_days = len(merged_df)
            annualized_factor = 365 / n_days if n_days > 0 else 0
        
        # è®¡ç®—å„ç»„åˆçš„å¹´åŒ–æ”¶ç›Šç‡
        long_annual_return = (np.exp(np.sum(merged_df['long_return'])) - 1) * annualized_factor
        short_annual_return = (np.exp(np.sum(merged_df['short_return'])) - 1) * annualized_factor
        long_short_annual_return = (np.exp(np.sum(merged_df['long_short_return'])) - 1) * annualized_factor
        market_annual_return = (np.exp(np.sum(merged_df['market_return'])) - 1) * annualized_factor
        
        # è®¡ç®—æ³¢åŠ¨ç‡
        long_volatility = np.std(merged_df['long_return']) * np.sqrt(365)
        short_volatility = np.std(merged_df['short_return']) * np.sqrt(365)
        long_short_volatility = np.std(merged_df['long_short_return']) * np.sqrt(365)
        market_volatility = np.std(merged_df['market_return']) * np.sqrt(365)
        
        # è®¡ç®—å¤æ™®æ¯”ç‡
        risk_free_rate = 0.02  # å‡è®¾æ— é£é™©åˆ©ç‡ä¸º2%
        long_sharpe = (long_annual_return - risk_free_rate) / long_volatility if long_volatility != 0 else 0
        short_sharpe = (short_annual_return - risk_free_rate) / short_volatility if short_volatility != 0 else 0
        long_short_sharpe = (long_short_annual_return - risk_free_rate) / long_short_volatility if long_short_volatility != 0 else 0
        market_sharpe = (market_annual_return - risk_free_rate) / market_volatility if market_volatility != 0 else 0
        
        # è®¡ç®—æœ€å¤§å›æ’¤
        try:
            long_cum_returns = np.exp(np.cumsum(merged_df['long_return'])) - 1
            short_cum_returns = np.exp(np.cumsum(merged_df['short_return'])) - 1
            long_short_cum_returns = np.exp(np.cumsum(merged_df['long_short_return'])) - 1
            market_cum_returns = np.exp(np.cumsum(merged_df['market_return'])) - 1
            
            long_max_drawdown = calculate_max_drawdown(long_cum_returns)
            short_max_drawdown = calculate_max_drawdown(short_cum_returns)
            long_short_max_drawdown = calculate_max_drawdown(long_short_cum_returns)
            market_max_drawdown = calculate_max_drawdown(market_cum_returns)
        except Exception as e:
            print(f"è®¡ç®—æœ€å¤§å›æ’¤æ—¶å‡ºé”™: {str(e)}")
            long_max_drawdown = np.nan
            short_max_drawdown = np.nan
            long_short_max_drawdown = np.nan
            market_max_drawdown = np.nan
        
        # åˆ›å»ºç»Ÿè®¡æŒ‡æ ‡DataFrame
        stats = pd.DataFrame({
            'Portfolio': ['Long', 'Short', 'Long-Short', 'Market'],
            'Annual Return': [long_annual_return, short_annual_return, long_short_annual_return, market_annual_return],
            'Annual Volatility': [long_volatility, short_volatility, long_short_volatility, market_volatility],
            'Sharpe Ratio': [long_sharpe, short_sharpe, long_short_sharpe, market_sharpe],
            'Max Drawdown': [long_max_drawdown, short_max_drawdown, long_short_max_drawdown, market_max_drawdown]
        })
        
        print(f"æŠ•èµ„ç»„åˆç»Ÿè®¡æŒ‡æ ‡è®¡ç®—å®Œæˆ: å¤šç©ºç»„åˆå¹´åŒ–æ”¶ç›Š={long_short_annual_return:.4f}, å¤æ™®æ¯”ç‡={long_short_sharpe:.4f}")
        return stats
        
    except Exception as e:
        print(f"è®¡ç®—æŠ•èµ„ç»„åˆç»Ÿè®¡æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame({
            'Portfolio': ['Long', 'Short', 'Long-Short', 'Market'],
            'Annual Return': [np.nan, np.nan, np.nan, np.nan],
            'Annual Volatility': [np.nan, np.nan, np.nan, np.nan],
            'Sharpe Ratio': [np.nan, np.nan, np.nan, np.nan],
            'Max Drawdown': [np.nan, np.nan, np.nan, np.nan]
        })

# è®¡ç®—æœ€å¤§å›æ’¤
def calculate_max_drawdown(cum_returns):
    """è®¡ç®—æœ€å¤§å›æ’¤"""
    running_max = np.maximum.accumulate(cum_returns)
    drawdown = (cum_returns - running_max) / (running_max + 1e-10)  # é¿å…é™¤ä»¥é›¶
    max_drawdown = np.min(drawdown)
    return max_drawdown

# è¿›è¡ŒCAPMå›å½’åˆ†æ
def perform_capm_analysis(portfolio_df, market_returns):
    """è¿›è¡ŒCAPMå›å½’åˆ†æ"""
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if portfolio_df.empty:
        print("è­¦å‘Š: æŠ•èµ„ç»„åˆæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒCAPMå›å½’åˆ†æ")
        return pd.DataFrame(), None
    
    if market_returns.empty:
        print("è­¦å‘Š: å¸‚åœºæ”¶ç›Šæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒCAPMå›å½’åˆ†æ")
        return pd.DataFrame(), None
    
    try:
        # åˆå¹¶æŠ•èµ„ç»„åˆæ”¶ç›Šå’Œå¸‚åœºæ”¶ç›Š
        merged_df = pd.merge(portfolio_df, market_returns, left_index=True, right_index=True, how='inner')
        
        # æ£€æŸ¥åˆå¹¶åçš„æ•°æ®
        if len(merged_df) < 10:  # è‡³å°‘éœ€è¦10ä¸ªæ•°æ®ç‚¹è¿›è¡Œæœ‰æ„ä¹‰çš„å›å½’
            print(f"è­¦å‘Š: æŠ•èµ„ç»„åˆå’Œå¸‚åœºæ”¶ç›Šåªæœ‰ {len(merged_df)} ä¸ªå…±åŒæ—¥æœŸï¼Œä¸è¶³ä»¥è¿›è¡Œå›å½’åˆ†æ")
            return pd.DataFrame(), None
        
        # è¿›è¡ŒCAPMå›å½’
        X = add_constant(merged_df['market_return'])
        y = merged_df['long_short_return']
        
        model = OLS(y, X).fit()
        
        # æå–å›å½’ç»“æœ
        alpha = model.params[0]
        beta = model.params[1]
        alpha_pvalue = model.pvalues[0]
        beta_pvalue = model.pvalues[1]
        r_squared = model.rsquared
        adj_r_squared = model.rsquared_adj
        alpha_significant = alpha_pvalue < 0.05
        
        # åˆ›å»ºç»“æœDataFrame
        capm_results = pd.DataFrame({
            'alpha': [alpha],
            'beta': [beta],
            'alpha_pvalue': [alpha_pvalue],
            'beta_pvalue': [beta_pvalue],
            'r_squared': [r_squared],
            'adj_r_squared': [adj_r_squared],
            'alpha_significant': [alpha_significant],
            'observations': [len(merged_df)]
        })
        
        print(f"CAPMå›å½’åˆ†æå®Œæˆ: Alpha={alpha:.6f} (p={alpha_pvalue:.4f}), Beta={beta:.4f}, RÂ²={r_squared:.4f}")
        return capm_results, model
        
    except Exception as e:
        print(f"CAPMå›å½’åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame(), None

# ç»˜åˆ¶CAPMå›å½’å›¾
def plot_capm_regression(portfolio_df, market_returns, model, title="CAPM Regression"):
    """ç»˜åˆ¶CAPMå›å½’å›¾"""
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if portfolio_df is None or portfolio_df.empty or market_returns is None or market_returns.empty or model is None:
        print(f"è­¦å‘Š: æ— æ³•ç»˜åˆ¶CAPMå›å½’å›¾ï¼Œæ•°æ®æˆ–æ¨¡å‹ä¸ºç©º")
        # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒ
        plt.figure(figsize=(10, 6))
        plt.title(f"{title}\n(æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œå›å½’åˆ†æ)")
        plt.xlabel('Market Return')
        plt.ylabel('Long-Short Portfolio Return')
        plt.grid(True)
        
        # ä¿å­˜ç©ºç™½å›¾åƒ
        save_path = os.path.join(output_dir, 'capm_regression.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… CAPMå›å½’å›¾(ç©ºç™½)å·²ä¿å­˜è‡³: {save_path}")
        return
    
    try:
        # åˆå¹¶æŠ•èµ„ç»„åˆæ”¶ç›Šå’Œå¸‚åœºæ”¶ç›Š
        merged_df = pd.merge(portfolio_df, market_returns, left_index=True, right_index=True, how='inner')
        
        if merged_df.empty:
            print(f"è­¦å‘Š: æŠ•èµ„ç»„åˆå’Œå¸‚åœºæ”¶ç›Šæ²¡æœ‰å…±åŒçš„æ—¥æœŸï¼Œæ— æ³•ç»˜åˆ¶CAPMå›å½’å›¾")
            plt.figure(figsize=(10, 6))
            plt.title(f"{title}\n(æ— å…±åŒæ—¥æœŸæ•°æ®)")
            plt.xlabel('Market Return')
            plt.ylabel('Long-Short Portfolio Return')
            plt.grid(True)
            
            # ä¿å­˜ç©ºç™½å›¾åƒ
            save_path = os.path.join(output_dir, 'capm_regression.png')
            plt.savefig(save_path)
            plt.close()
            print(f"âœ… CAPMå›å½’å›¾(æ— å…±åŒæ—¥æœŸ)å·²ä¿å­˜è‡³: {save_path}")
            return
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾å’Œå›å½’çº¿
        plt.figure(figsize=(10, 6))
        plt.scatter(merged_df['market_return'], merged_df['long_short_return'], alpha=0.5)
        
        # æ·»åŠ å›å½’çº¿
        x_range = np.linspace(merged_df['market_return'].min(), merged_df['market_return'].max(), 100)
        y_pred = model.params[0] + model.params[1] * x_range
        plt.plot(x_range, y_pred, 'r-', linewidth=2)
        
        # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
        alpha = model.params[0]
        beta = model.params[1]
        r_squared = model.rsquared
        alpha_pvalue = model.pvalues[0]
        
        plt.title(f"{title}\nAlpha: {alpha:.4f} (p-value: {alpha_pvalue:.4f}), Beta: {beta:.4f}, RÂ²: {r_squared:.4f}")
        plt.xlabel('Market Return')
        plt.ylabel('Long-Short Portfolio Return')
        plt.grid(True)
        
        # æ·»åŠ é›¶çº¿
        plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
        plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
        
        # ä¿å­˜å›¾è¡¨
        save_path = os.path.join(output_dir, 'capm_regression.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… CAPMå›å½’å›¾å·²ä¿å­˜è‡³: {save_path}")
    except Exception as e:
        print(f"ç»˜åˆ¶CAPMå›å½’å›¾æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # åˆ›å»ºä¸€ä¸ªé”™è¯¯ä¿¡æ¯å›¾åƒ
        plt.figure(figsize=(10, 6))
        plt.title(f"{title}\n(ç»˜å›¾è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)})")
        plt.xlabel('Market Return')
        plt.ylabel('Long-Short Portfolio Return')
        plt.grid(True)
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯å›¾åƒ
        save_path = os.path.join(output_dir, 'capm_regression.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… CAPMå›å½’å›¾(é”™è¯¯ä¿¡æ¯)å·²ä¿å­˜è‡³: {save_path}")

# ç»˜åˆ¶æ¨¡å‹ç‰¹å¾é‡è¦æ€§å›¾
def plot_feature_importance(model_weights, factors_data, model_type='lasso', top_n=20):
    """ç»˜åˆ¶æ¨¡å‹ç‰¹å¾é‡è¦æ€§å›¾"""
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if model_weights is None or len(model_weights) == 0:
        print("è­¦å‘Š: æ¨¡å‹æƒé‡ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾")
        # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒ
        plt.figure(figsize=(12, 8))
        plt.title(f"{model_type.capitalize()} æ¨¡å‹ç‰¹å¾é‡è¦æ€§\n(æ— æ¨¡å‹æƒé‡æ•°æ®)")
        plt.xlabel('é‡è¦æ€§')
        plt.ylabel('ç‰¹å¾')
        plt.grid(True)
        
        # ä¿å­˜ç©ºç™½å›¾åƒ
        output_dir = os.environ.get('OUTPUT_DIR', 'ml_strategy_results')
        save_path = os.path.join(output_dir, f'feature_importance_{model_type}.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾(ç©ºç™½)å·²ä¿å­˜è‡³: {save_path}")
        return
    
    if factors_data is None or len(factors_data) == 0:
        print("è­¦å‘Š: å› å­æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾")
        # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒ
        plt.figure(figsize=(12, 8))
        plt.title(f"{model_type.capitalize()} æ¨¡å‹ç‰¹å¾é‡è¦æ€§\n(æ— å› å­æ•°æ®)")
        plt.xlabel('é‡è¦æ€§')
        plt.ylabel('ç‰¹å¾')
        plt.grid(True)
        
        # ä¿å­˜ç©ºç™½å›¾åƒ
        output_dir = os.environ.get('OUTPUT_DIR', 'ml_strategy_results')
        save_path = os.path.join(output_dir, f'feature_importance_{model_type}.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾(ç©ºç™½)å·²ä¿å­˜è‡³: {save_path}")
        return
    
    try:
        # è·å–æœ€åä¸€ä¸ªæ¨¡å‹çš„æƒé‡
        weights = model_weights[-1]
        
        # è·å–ç‰¹å¾åç§°
        feature_names = []
        for factor_name, factor_df in factors_data:
            for col in factor_df.columns:
                token = col.split('_')[0]  # æå–ä»£å¸åç§°
                feature_names.append(f"{factor_name}_{token}")
        
        # æ£€æŸ¥ç‰¹å¾åç§°å’Œæƒé‡é•¿åº¦æ˜¯å¦åŒ¹é…
        if len(feature_names) != len(weights):
            print(f"è­¦å‘Š: ç‰¹å¾åç§°æ•°é‡({len(feature_names)})ä¸æƒé‡æ•°é‡({len(weights)})ä¸åŒ¹é…")
            # è°ƒæ•´é•¿åº¦ä»¥åŒ¹é…
            min_len = min(len(feature_names), len(weights))
            feature_names = feature_names[:min_len]
            weights = weights[:min_len]
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        if model_type == 'lasso':
            importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': np.abs(weights)
            })
        elif model_type == 'lightgbm':
            importance_df = pd.DataFrame({
                'Feature': feature_names,  # å·²ç¡®ä¿é•¿åº¦åŒ¹é…
                'Importance': weights
            })
        
        # æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©å‰top_nä¸ªç‰¹å¾
        importance_df = importance_df.sort_values('Importance', ascending=False).head(top_n)
        
        # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
        plt.figure(figsize=(12, 8))
    except Exception as e:
        print(f"å‡†å¤‡ç‰¹å¾é‡è¦æ€§æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # åˆ›å»ºä¸€ä¸ªé”™è¯¯ä¿¡æ¯å›¾åƒ
        plt.figure(figsize=(12, 8))
        plt.title(f"{model_type.capitalize()} æ¨¡å‹ç‰¹å¾é‡è¦æ€§\n(å¤„ç†æ•°æ®æ—¶å‡ºé”™: {str(e)})")
        plt.xlabel('é‡è¦æ€§')
        plt.ylabel('ç‰¹å¾')
        plt.grid(True)
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯å›¾åƒ
        output_dir = os.environ.get('OUTPUT_DIR', 'ml_strategy_results')
        save_path = os.path.join(output_dir, f'feature_importance_{model_type}.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾(é”™è¯¯ä¿¡æ¯)å·²ä¿å­˜è‡³: {save_path}")
        return
    try:
        # ç»˜åˆ¶æ¡å½¢å›¾
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        
        plt.title(f'Top {top_n} Feature Importance ({model_type.capitalize()} Model)')
        plt.xlabel('Importance')
        plt.ylabel('Feature')
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        save_path = os.path.join(output_dir, f'{model_type}_feature_importance.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")
    except Exception as e:
        print(f"ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # åˆ›å»ºä¸€ä¸ªé”™è¯¯ä¿¡æ¯å›¾åƒ
        plt.figure(figsize=(12, 8))
        plt.title(f"{model_type.capitalize()} æ¨¡å‹ç‰¹å¾é‡è¦æ€§\n(ç»˜å›¾è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)})")
        plt.xlabel('é‡è¦æ€§')
        plt.ylabel('ç‰¹å¾')
        plt.grid(True)
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯å›¾åƒ
        save_path = os.path.join(output_dir, f'{model_type}_feature_importance.png')
        plt.savefig(save_path)
        plt.close()
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾(é”™è¯¯ä¿¡æ¯)å·²ä¿å­˜è‡³: {save_path}")

# ä¸»å‡½æ•°
def main():
    try:
        print("\nğŸ“Š å¼€å§‹æœºå™¨å­¦ä¹ å› å­ç­–ç•¥å›æµ‹")
        
        # åŠ è½½å› å­æ•°æ®
        print("\nğŸ” åŠ è½½å› å­æ•°æ®...")
        factors = get_available_factors()
        factors_data = [(factor_name, load_factor_data(factor_path)) for factor_name, factor_path in factors]
        print(f"âœ… å·²åŠ è½½ {len(factors_data)} ä¸ªå› å­")
        
        # åŠ è½½æ”¶ç›Šæ•°æ®
        print("\nğŸ” åŠ è½½æ”¶ç›Šæ•°æ®...")
        return_data = load_return_data(return_period)
        print(f"âœ… å·²åŠ è½½ {return_period} æ”¶ç›Šæ•°æ®ï¼Œå…± {len(return_data)} æ¡è®°å½•")
        
        # åŠ è½½å¸‚åœºæ”¶ç›Šæ•°æ®
        print("\nğŸ” åŠ è½½å¸‚åœºæ”¶ç›Šæ•°æ®...")
        market_returns = load_market_return_data(return_period)
        print(f"âœ… å·²åŠ è½½å¸‚åœºæ”¶ç›Šæ•°æ®ï¼Œå…± {len(market_returns)} æ¡è®°å½•")
        
        # æ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹
        print(f"\nğŸ” ä½¿ç”¨ {model_type.capitalize()} æ¨¡å‹è¿›è¡Œæ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹...")
        predictions, model_weights = rolling_train_predict(
            factors_data, 
            return_data, 
            model_type=model_type,
            rolling_window=rolling_window,
            prediction_window=prediction_window
        )
        print(f"âœ… é¢„æµ‹å®Œæˆï¼Œå…± {len(predictions)} æ¡é¢„æµ‹è®°å½•")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        predictions_file = os.path.join(output_dir, f'predictions_{model_type}_{return_period}.csv')
        predictions.to_csv(predictions_file)
        print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {predictions_file}")
        
        # æ„å»ºå¤šç©ºç»„åˆ
        print("\nğŸ” æ„å»ºå¤šç©ºç»„åˆ...")
        portfolio_df = build_long_short_portfolio(predictions, top_pct=top_pct, bottom_pct=bottom_pct)
        print(f"âœ… å¤šç©ºç»„åˆæ„å»ºå®Œæˆï¼Œå…± {len(portfolio_df)} ä¸ªäº¤æ˜“æ—¥")
        
        # è®¡ç®—ç´¯ç§¯æ”¶ç›Š
        portfolio_df = calculate_cumulative_returns(portfolio_df)
        
        # ä¿å­˜ç»„åˆæ”¶ç›Š
        portfolio_file = os.path.join(output_dir, f'portfolio_returns_{model_type}_{return_period}.csv')
        portfolio_df.to_csv(portfolio_file)
        print(f"âœ… ç»„åˆæ”¶ç›Šå·²ä¿å­˜è‡³: {portfolio_file}")
        
        # ç»˜åˆ¶ç´¯ç§¯æ”¶ç›Šå›¾
        plot_cumulative_returns(portfolio_df, title=f"{model_type.capitalize()} Model Strategy Cumulative Returns ({return_period})")
        
        # è®¡ç®—æŠ•èµ„ç»„åˆç»Ÿè®¡æŒ‡æ ‡
        print("\nğŸ” è®¡ç®—æŠ•èµ„ç»„åˆç»Ÿè®¡æŒ‡æ ‡...")
        stats = calculate_portfolio_stats(portfolio_df, market_returns)
        
        # ä¿å­˜ç»Ÿè®¡æŒ‡æ ‡
        stats_file = os.path.join(output_dir, f'portfolio_stats_{model_type}_{return_period}.csv')
        stats.to_csv(stats_file, index=False)
        print(f"âœ… æŠ•èµ„ç»„åˆç»Ÿè®¡æŒ‡æ ‡å·²ä¿å­˜è‡³: {stats_file}")
        print(stats)
        
        # è¿›è¡ŒCAPMå›å½’åˆ†æ
        print("\nğŸ” è¿›è¡ŒCAPMå›å½’åˆ†æ...")
        capm_results, capm_model = perform_capm_analysis(portfolio_df, market_returns)
        
        # ä¿å­˜CAPMå›å½’ç»“æœ
        capm_file = os.path.join(output_dir, f'capm_results_{model_type}_{return_period}.csv')
        capm_results.to_csv(capm_file, index=False)
        print(f"âœ… CAPMå›å½’ç»“æœå·²ä¿å­˜è‡³: {capm_file}")
        print(capm_results)
        
        # ç»˜åˆ¶CAPMå›å½’å›¾
        plot_capm_regression(portfolio_df, market_returns, capm_model, title=f"{model_type.capitalize()} Model Strategy CAPM Regression ({return_period})")
        
        # ç»˜åˆ¶æ¨¡å‹ç‰¹å¾é‡è¦æ€§å›¾
        plot_feature_importance(model_weights, factors_data, model_type=model_type)
        
        print("\nâœ… æœºå™¨å­¦ä¹ å› å­ç­–ç•¥å›æµ‹å®Œæˆ!")
        
        return portfolio_df, stats, capm_results, model_weights
    except Exception as e:
        print(f"\nâŒ ç­–ç•¥æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

if __name__ == "__main__":
    main()